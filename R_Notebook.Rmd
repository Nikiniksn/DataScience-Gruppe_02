---
title: "R Notebook"
output: html_notebook
---
 
# Einbinden benötigter Bibliotheken
```{r}
library(readr)
library(ggplot2)
library(lubridate)
library(Metrics)
library(e1071)
library(dplyr)
library(broom)
library(fastDummies)
```

# Installation von Python und der für TensorFlow benötigten Pakete (nur einmalig nötig)
```{r}
install.packages("reticulate")
library(reticulate)

# Installation von miniconda (falls nicht vorhanden)
install_miniconda()

# Anlegen einer speziellen Python Umgebung
conda_create("r-reticulate")

# Installieren der Pakete in der angelegten Umgebung
conda_install("r-reticulate", "pandas")
conda_install("r-reticulate", "numpy")
conda_install("r-reticulate", "tensorflow")

# Verwenden der speziellen Python Umgebung die zuvor erstellt wurde
use_condaenv("r-reticulate")
```

# Einlesen der CSV-Datein
```{r}
Umsatzdaten <- read_csv("umsatzdaten_gekuerzt.csv")

Kiwo <- read_csv("kiwo.csv")

Feiertage <- read_csv("Feiertage.csv")

Schulferien <- read_csv("Schulferien.csv")

Wetter <- read_csv("wetter.csv")
```

# Integrierung der Wochentage in die Datei Umsatzdaten
```{r}
Umsatzdaten$Wochentag <- weekdays(Umsatzdaten$Datum)

# Umwandlung von 'wochentag" in eine Faktor-Variable mit einer vorgegeben Sortierung der Level (Kategorien)
Umsatzdaten$Wochentag <- factor(Umsatzdaten$Wochentag, levels=c("Montag", "Dienstag", "Mittwoch", "Donnerstag", "Freitag", "Samstag", "Sonntag"))
```

# Zusammenführung der Datein zu einer Tabelle
```{r}
Sammlung <- dplyr::full_join(Umsatzdaten,Wetter,by="Datum")
Sammlung <- dplyr::full_join(Sammlung,Kiwo,by="Datum")
Sammlung <- dplyr::full_join(Sammlung,Schulferien,by="Datum")
Sammlung <- dplyr::full_join(Sammlung,Feiertage,by="Datum")
```

## Aufteilung des Datensatzes in Trainings- und Testdaten
```{r}
# Zufallszähler setzen (um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten)
set.seed(1)

# Zufällige Ziehung Indizes für die Zeilen des Datensatzes, die dem Traininsdatensatz zugeordnet werden
indices_train <- sample(seq_len(nrow(Sammlung)), size = floor(0.80 * nrow(Sammlung)))

# Definition des Trainings- und Testdatensatz durch Selektion bzw. Deselektion der entsprechenden Datenzeilen
train_dataset <- train_dataset_org <- Sammlung[indices_train, ]
test_dataset <- Sammlung[-indices_train, ]
```

# Vorhersage durch Wetterdaten#
```{r}
mod <- lm (Umsatz ~ Temperatur + Windgeschwindigkeit + as.factor(Wettercode) + as.factor (Bewoelkung),Sammlung) 
summary(mod)
#Kein signifikanter Einfluss der Windgeschwindigkeit auf Umsatz (eventuell u-förmigen Effekt prüfen)
#Signifikant positiver Zusammenhang zwischen Temperatur und Umsatz: Bei höheren Temperaturen ist der Umsatz höher# 
#Signifikant geringerer Umsatz bei Wettercode 71 -->  Bei durchgehendem leichten Schneefall weniger gekauft#
#Bewölkung Code 1 sagt Umsatz signifikant vorher: höherer Umsatz bei wolkenlosem Himmel, evtl Bildung dichotomer Variable sonnig vs. bewölkt###
```

# Vohersage durch Warengruppe#
```{r}
mod <-lm (Umsatz ~ as.factor (Warengruppe),Sammlung) 
summary (mod)
#Einzelne Warengruppen in signifikantem Zusammenhang mit Umsatz (logisch, Aufnahme nur bedingt sinnvoll)
#Warengruppe 2,3 und 5 sagen hohen Umsatz vorher, Warengruppe 4 und 6 (konditorei und Saisonbrot) korrelieren signifikant negativ mit Umsatz
```

# Vohersage durch Ferien###
```{r}
mod <- lm (Umsatz ~ Ferien, Sammlung)
summary (mod)
###Signifikant höherer Umsatz in Sommerferien###
```

# Vorhersage durch Feiertage
```{r}
mod <- lm (Umsatz ~ Feiertag, Sammlung)
summary (mod)
###kein signifikanter Einfluss der Feiertage, 
##eventuell Prüfung in Abhängigkeit von Ferien und langem Wochenende###
```

# Vorhersage durch Kieler Woche
```{r}
mod <-lm (Umsatz ~ KielerWoche, Sammlung)
summary (mod)
#Kieler Woche wirkt sich signifikant positiv auf Umsatz aus, starker Effekt**
```

# Vorhersage durch Wochentag
```{r}
mod <-lm (Umsatz ~ as.factor(Wochentag), Umsatzdaten)
summary(mod)
#Signifikant erhöhter Umsatz am Wochenende###

###Fazit: bedeutsame Haupteffekte###:

##positiv:

##Kieler Woche##
##Wochenende##
##Sommerferien##
##Sonniges Wetter##
#Verkauf Brot, Brötchen und Croissants##
##Hohe Temperaturen##

##negativ
##leichter Schneefall
##Verkauf von Saisonbrot und Konditoreiware (evtl. geringere Marge)
```

# Vorhersage durch Wetterdaten#
```{r}
mod <- lm (Umsatz ~ Windgeschwindigkeit + Temperatur,Sammlung) 
summary(mod)
```

# Estimating (Training) Models
# Nur zufällige Kombination von Faktoren
# To Do: Systematisierung der Faktoren und Integration von Wochentages, Feiertagen und Ferien###
```{r}
mod1 <- lm(Umsatz ~ Temperatur, Sammlung)
mod2 <- lm(Umsatz ~ Temperatur + Windgeschwindigkeit, Sammlung)
mod3 <- lm(Umsatz ~ Temperatur + Windgeschwindigkeit + as.factor(Bewoelkung), Sammlung)
mod4 <- lm(Umsatz ~ Temperatur + Windgeschwindigkeit + as.factor(Bewoelkung) + as.factor(Wettercode), Sammlung)
mod5 <- lm(Umsatz ~ Temperatur + Windgeschwindigkeit + as.factor(Bewoelkung) + as.factor(Wettercode) + as.factor(Warengruppe), Sammlung)
mod6 <- lm(Umsatz ~ Temperatur + Windgeschwindigkeit + as.factor(Bewoelkung) + as.factor(Wettercode) + as.factor(Warengruppe) + KielerWoche, Sammlung)
mod7 <- lm(Umsatz ~ Temperatur + Windgeschwindigkeit + Windgeschwindigkeit*Temperatur + as.factor(Bewoelkung) + as.factor(Wettercode) + as.factor(Warengruppe) + KielerWoche , Sammlung)
```

```{r}
summary(mod1)
summary (mod2)
summary(mod3)
summary(mod3)
summary(mod4)
summary(mod5)
summary(mod6)
summary(mod7)
```

```{r}
glance(mod1)
glance (mod2)
glance (mod3)
glance (mod4)
glance (mod5)
glance (mod6)
glance (mod7)
```

# Preparation of Model Results
```{r}
rbind(glance(mod1), glance(mod2), glance(mod3), glance(mod4), glance(mod5), glance(mod6), glance(mod7))
```

#Vorhersageverbesserung vor allem durch Aufnahme von Warengruppe und Kieler Woche,
#Aufnahme von Warengruppe jedoch nur bedingt sinnvoll#
#Kieler Woche als sehr starker Umsatztreiber###
##To Do: Unbedingt Wochentag und Ferien intergieren, da sign. Korrelation mit Umsatz in einfacher Regression#
```{r}
# Model Prediction Quality for the Training Data Using the Mean Absolute Error
rbind(mae(Sammlung$Umsatz, predict(mod1)),
      mae(Sammlung$Umsatz, predict(mod2)),
      mae(Sammlung$Umsatz, predict(mod3)),
      mae(Sammlung$Umsatz, predict(mod4)),
      mae(Sammlung$Umsatz, predict(mod5)),
      mae(Sammlung$Umsatz, predict(mod6)),
      mae(Sammlung$Umsatz, predict(mod7)))
```

```{r}
# Model Prediction Quality for the Training Data Using the Mean Absolute Percentage Error
rbind(mape(Sammlung$Umsatz, predict(mod1)),
       mape(Sammlung$Umsatz, predict(mod2)),
       mape(Sammlung$Umsatz, predict(mod3)),
       mape(Sammlung$Umsatz, predict(mod4)),
       mape(Sammlung$Umsatz, predict(mod5)),
       mape(Sammlung$Umsatz, predict(mod6)),
       mape(Sammlung$Umsatz, predict(mod7)))
```

```{r}
# Model Prediction Quality for the (Unknown) Test Data Using the Mean Absolute Percentage Error
rbind(mape(Sammlung_test$Umsatz, predict(mod1, newdata=Sammlung_test)),
      mape(Sammlung_test$Umsatz, predict(mod2, newdata=Sammlung_test)),
      mape(Sammlung_test$Umsatz, predict(mod3, newdata=Sammlung_test)),
      mape(Sammlung_test$Umsatz, predict(mod4, newdata=Sammlung_test)),
      mape(Sammlung_test$Umsatz, predict(mod5, newdata=Sammlung_test)),
      mape(Sammlung_test$Umsatz, predict(mod6, newdata=Sammlung_test)))
```

## Data Preparation
```{r}
# Uncomment the following line to check the correctness of the code with a small (and computationally fast) training data set
train_dataset <- sample_frac(train_dataset_org, .10)
```

## Training the SVM
```{r}
# Estimation of an SVM with optimized weighting parameters and given standard hyper parameters
# Typically not used; instead, the function svm_tune is used in order to also get a model with optimized hyper parameters
model_svm <- svm(Umsatz ~ Wochentag, train_dataset)
```

```{r}
# Estimation of various SVM (each with optimized weighting parameters) using systematically varied hyper parameters (typically called 'grid search' approach) and cross validation
# the resulting object includes the optimal model in the element named 'best.model'
svm_tune <- tune(svm, Umsatz ~ Wochentag + Bewoelkung + Temperatur + Ferien, data=train_dataset,
                 ranges = list(epsilon = seq(0.2,1,0.1), cost = 2^(2:3)))
```

## Checking the Prediction Quality
```{r}
# Calculating the prediction for the training data using the best model according to the grid search
pred_train <- predict(svm_tune$best.model, train_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(train_dataset$price, pred_train)
```

```{r}
# Calculating the prediction for the test data using the best model according to the grid search
pred_test <- predict(svm_tune$best.model, test_dataset)
# Calculating the prediction quality for the training data using the MAPE
mape(test_dataset$Umsatz, pred_test)
```

## Neuronales Netz
### Funktionsdefinitionen ###
```{r}
#' Title Fast creation of normalized variables
#' Quickly create normalized columns from numeric type columns in the input data. This function is useful for statistical analysis when you want normalized columns rather than the actual columns.
#'
#' @param .data An object with the data set you want to make normalized columns from.
#' @param norm_values Dataframe of column names, means, and standard deviations that is used to create corresponding normalized variables from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) with same number of rows an dcolumns as the inputted data, only with normalized columns for the variables indicated in the norm_values argument.
#' @export
#'
#' @examples
norm_cols <- function (.data, norm_values = NULL) {
  for (i in 1:nrow(norm_values)  ) {
    .data[[norm_values$name[i]]] <- (.data[[norm_values$name[i]]] - norm_values$mean[i]) / norm_values$sd[i]
  }
  return (.data)
}


#' Title Creation of a Dataframe including the Information to Standardize Variables
#' This function is meant to be used in combination with the function norm_cols
#'
#' @param .data A data set including the variables you want to get the means and standard deviations from.
#' @param select_columns A vector with a list of variable names for which you want to get the means and standard deviations from.
#'
#' @return A data.frame (or tibble or data.table, depending on input data type) including the names, means, and standard deviations of the variables included in the select_columns argument.
#' @export
#'
#' @examples
get.norm_values <- function (.data, select_columns = NULL) {
  result <- NULL
  for (col_name in select_columns) {
    mean <- mean(.data[[col_name]], na.rm = TRUE)
    sd <- sd(.data[[col_name]], na.rm = TRUE)
    result <- rbind (result, c(mean, sd))
  }
  result <- as.data.frame(result, stringsAsFactors = FALSE)
  result <- data.frame (select_columns, result, stringsAsFactors = FALSE)
  names(result) <- c("name", "mean", "sd")
  return (result)
}

```

# Datenaufbereitung
```{r}
# Rekodierung von kategoriellen Variablen (zu Dummy-Variablen)
dummy_list <- c("view", "condition")
house_pricing_dummy = dummy_cols(house_pricing, dummy_list)

# Definition von Variablenlisten für die Dummies, um das Arbeiten mit diesen zu erleichtern
condition_dummies = c('condition_1', 'condition_2', 'condition_3', 'condition_4', 'condition_5')
view_dummies = c('view_0', 'view_1', 'view_2', 'view_3','view_4')


# Standardisierung aller Feature Variablen und der Label Variable
norm_list <- c("price", "sqft_lot", "bathrooms", "grade", "waterfront", view_dummies, condition_dummies) # Liste aller Variablen
norm_values_list <- get.norm_values(house_pricing_dummy, norm_list)    # Berechnung der Mittelwerte und Std.-Abw. der Variablen
house_pricing_norm <- norm_cols(house_pricing_dummy, norm_values_list) # Standardisierung der Variablen
```

# Definition der Feaure-Variablen und der Label-Variable
```{r}
# Definition der Features (der unabhängigen Variablen auf deren Basis die Vorhersagen erzeugt werden sollen)
features = c('sqft_lot', 'waterfront', 'grade', 'bathrooms', view_dummies, condition_dummies)
# Definition der Label-Variable (der abhaengigen Variable, die vorhergesagt werden soll) sowie
label = 'price_norm'
```

# Definition von Trainings- und Testdatensatz
```{r}
# Zufallszähler setzen, um die zufällige Partitionierung bei jedem Durchlauf gleich zu halten
set.seed(1)
# Bestimmung der Indizes des Traininsdatensatzes
train_ind <- sample(seq_len(nrow(house_pricing_norm)), size = floor(0.80 * nrow(house_pricing_norm)))

# Teilen in Trainings- und Testdatensatz
train_dataset = house_pricing_norm[train_ind, features]
test_dataset = house_pricing_norm[-train_ind, features]

# Selektion der Variable, die als Label definiert wurde
train_labels = house_pricing_norm[train_ind, label]
test_labels = house_pricing_norm[-train_ind, label]
```

